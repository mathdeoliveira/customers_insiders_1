{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999fe9a-2b99-472a-9ae0-e96256a2f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_FLAG = \"--user\"\n",
    "# !pip3 install {USER_FLAG} kfp --upgrade\n",
    "# !pip3 install {USER_FLAG} google_cloud_pipeline_components --upgrade\n",
    "# !pip3 install {USER_FLAG} 'apache-beam[gcp]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62951f29-f121-41de-bb1b-d38d2ad93ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.22\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a77173-e6d4-474c-8e90-30604f591821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.25.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"from google.cloud import aiplatform; print('aiplatform SDK version: {}'.format(aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d1b66c-efd4-4704-8598-59853ec855d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (pipeline,\n",
    "                        Artifact,\n",
    "                        Model,\n",
    "                        Input,\n",
    "                        Output,\n",
    "                        component)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a047a3-794c-4401-a587-3a20cc8083ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e4d335-3df8-43c8-beeb-fd307efe9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-bigquery\", \"db-dtypes\", \"pandas\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"get_data_ecommerce.yaml\")\n",
    "def get_data():\n",
    "    import logging\n",
    "    \n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_RAW_ID = 'dados_ecommerce_raw'\n",
    "    TABLE_ID = 'ecommerce_cds'\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to execute in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query,  or error, if any\n",
    "        \"\"\"\n",
    "        \n",
    "        bq_client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "        bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        # If dry run succeeds without errors, proceed to run query\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        job_id = client_result.job_id\n",
    "\n",
    "        # Wait for query/job to finish running. then get & return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job_id: {job_id}\")\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    query = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE\n",
    "               `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` (InvoiceNo STRING,\n",
    "                StockCode STRING,\n",
    "                Description STRING,\n",
    "                Quantity INT64,\n",
    "                InvoiceDate DATE,\n",
    "                UnitPrice FLOAT64,\n",
    "                CustomerID FLOAT64,\n",
    "                Country STRING)\n",
    "            PARTITION BY\n",
    "              InvoiceDate AS (\n",
    "              WITH\n",
    "                not_nulls AS (\n",
    "                SELECT\n",
    "                  *\n",
    "                FROM\n",
    "                  `{PROJECT_ID}.{DATASET_ID}.{TABLE_RAW_ID}`\n",
    "                WHERE\n",
    "                  InvoiceDate <= CURRENT_DATE()\n",
    "                  AND CustomerID IS NOT NULL\n",
    "                  AND Description IS NOT NULL),\n",
    "                filtering_features AS (\n",
    "                SELECT\n",
    "                  *\n",
    "                FROM\n",
    "                  not_nulls\n",
    "                WHERE\n",
    "                  UnitPrice >= 0.04\n",
    "                  AND Country NOT IN ('European Community',\n",
    "                    'Unspecified')\n",
    "                  AND StockCode NOT IN ('POST',\n",
    "                    'D',\n",
    "                    'DOT',\n",
    "                    'M',\n",
    "                    'S',\n",
    "                    'AMAZONFEE',\n",
    "                    'm',\n",
    "                    'DCGSSBOY',\n",
    "                    'DCGSSGIRL',\n",
    "                    'PADS',\n",
    "                    'B',\n",
    "                    'CRUK')\n",
    "                  AND CustomerID != 16446)\n",
    "              SELECT\n",
    "                *\n",
    "              FROM\n",
    "                filtering_features);\n",
    "    \"\"\"\n",
    "    \n",
    "    run_bq_query(query, project_name=PROJECT_ID)\n",
    "    logging.info(f'Tabela criada: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b625980-4fcd-4d91-b2e0-1d6808355d09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e66866e-36ff-455f-87f8-b898e0473158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-bigquery\", \"db-dtypes\", \"pandas-gbq\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"data_preparation_ecommerce.yaml\")\n",
    "def data_preparation():\n",
    "    import os\n",
    "    import logging\n",
    "    from typing import Tuple\n",
    "\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    \n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_RAW_ID = 'dados_ecommerce_raw'\n",
    "    TABLE_ID = 'ecommerce_cds'\n",
    "    TABLE_FILTERED_TEMP_ID = 'temp_data_filtered'\n",
    "    TABLE_PURCHASES_TEMP_ID = 'temp_data_purchases'\n",
    "    TABLE_RETURNS_TEMP_ID = 'temp_data_returns'\n",
    "    PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "\n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_columns.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "        return dataframe[keep_columns]\n",
    "    def column_to_int(dataframe: pd.DataFrame, column_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Converte a coluna especificada em um dataframe para o tipo inteiro.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O dataframe a ser processado.\n",
    "            column_name (str): O nome da coluna a ser convertida.\n",
    "\n",
    "        Returns:\n",
    "            bool: True se a conversão foi bem sucedida, False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe[column_name] = dataframe[column_name].astype(int)\n",
    "        except (ValueError, TypeError):\n",
    "            # Lidar com valores ausentes e conversões inválidas\n",
    "            return False\n",
    "\n",
    "        # Retorna True se a conversão foi bem sucedida\n",
    "        return True\n",
    "    \n",
    "    def column_to_date(dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Converte a coluna especificada em um dataframe para o tipo data.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O dataframe a ser processado.\n",
    "            column_name (str): O nome da coluna a ser convertida.\n",
    "            date_format (str, opcional): O formato de data personalizado. Se nenhum formato for especificado, o pandas usará o padrão 'YYYY-MM-DD'.\n",
    "\n",
    "        Returns:\n",
    "            bool: True se a conversão foi bem sucedida, False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format=date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            # Lidar com valores ausentes e conversões inválidas\n",
    "            return False\n",
    "\n",
    "        # Retorna True se a conversão foi bem sucedida\n",
    "        return True\n",
    "\n",
    "\n",
    "    def change_column_type(dataframe_raw: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Changes the data type of a given column in a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        column_to_int(dataframe_raw, 'CustomerID')\n",
    "        column_to_date(dataframe_raw, 'InvoiceDate')\n",
    "\n",
    "    def filtering_features(dataframe_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Filters and preprocesses the input dataframe.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas DataFrame containing raw sales data.\n",
    "\n",
    "        Returns:\n",
    "            Three pandas DataFrames containing the filtered returns and purchases data, and the filtered main data.\n",
    "        \"\"\"\n",
    "        # Filter returns and purchases data\n",
    "        df_returns = dataframe_raw.loc[dataframe_raw['Quantity'] < 0, ['CustomerID', \n",
    "                                                                       'Quantity']]\n",
    "        df_purchases = dataframe_raw.loc[dataframe_raw['Quantity'] >= 0, :]\n",
    "\n",
    "        # Filter main data\n",
    "        df_filtered = keep_features(dataframe_raw, ['InvoiceNo', 'StockCode', 'Quantity',\n",
    "                                                    'InvoiceDate', 'UnitPrice', \n",
    "                                                    'CustomerID', 'Country'])\n",
    "\n",
    "        return df_filtered, df_purchases, df_returns\n",
    "\n",
    "    def run_data_preparation(dataframe_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocesses the input dataframe by performing column type conversion and filtering features.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw (pd.DataFrame): A pandas DataFrame containing raw sales data.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three pandas DataFrames: df_filtered, df_purchases, and df_returns.\n",
    "            - df_filtered: A DataFrame containing the filtered main data.\n",
    "            - df_purchases: A DataFrame containing the filtered purchases data.\n",
    "            - df_returns: A DataFrame containing the filtered returns data.\n",
    "        \"\"\"\n",
    "        change_column_type(dataframe_raw)\n",
    "        return filtering_features(dataframe_raw)\n",
    "    \n",
    "    query_sql = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "                    WHERE InvoiceDate <= CURRENT_DATE \"\"\"\n",
    "    \n",
    "    data = pd.read_gbq(query=query_sql, \n",
    "                         project_id=PROJECT_NUMBER) \n",
    "    logging.info(f'Tabela carregada: `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`')\n",
    "    \n",
    "    df_filtered, df_purchases, df_returns = run_data_preparation(data)\n",
    "    \n",
    "    pandas_gbq.to_gbq(df_filtered, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_FILTERED_TEMP_ID}', project_id=PROJECT_NUMBER, if_exists='replace')\n",
    "    pandas_gbq.to_gbq(df_purchases, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_PURCHASES_TEMP_ID}', project_id=PROJECT_NUMBER, if_exists='replace')\n",
    "    pandas_gbq.to_gbq(df_returns, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_RETURNS_TEMP_ID}', project_id=PROJECT_NUMBER, if_exists='replace')\n",
    "    \n",
    "    logging.info(f'Tabelas criadas no BigQuery: {TABLE_FILTERED_TEMP_ID} e {TABLE_PURCHASES_TEMP_ID} e {TABLE_RETURNS_TEMP_ID}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c69b6a8-d5c8-4e70-a204-9bb2f9bf24a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b01d43f-ed4d-406d-b410-136ddbe4e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-bigquery\", \"db-dtypes\", \"pandas-gbq\", \"google-cloud\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"feature_engineering_ecommerce.yaml\")\n",
    "def feature_engineering():\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import os\n",
    "    import logging\n",
    "    from functools import reduce\n",
    "    from typing import Union\n",
    "    \n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_ID = 'dados_engenharia_features'\n",
    "    TABLE_RAW_ID = 'dados_ecommerce_raw'\n",
    "    TABLE_FILTERED_TEMP_ID = 'temp_data_filtered'\n",
    "    TABLE_PURCHASES_TEMP_ID = 'temp_data_purchases'\n",
    "    TABLE_RETURNS_TEMP_ID = 'temp_data_returns'\n",
    "    PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    def column_to_date(dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Converte a coluna especificada em um dataframe para o tipo data.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O dataframe a ser processado.\n",
    "            column_name (str): O nome da coluna a ser convertida.\n",
    "            date_format (str, opcional): O formato de data personalizado. Se nenhum formato for especificado, o pandas usará o padrão 'YYYY-MM-DD'.\n",
    "\n",
    "        Returns:\n",
    "            bool: True se a conversão foi bem sucedida, False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format=date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            # Lidar com valores ausentes e conversões inválidas\n",
    "            return False\n",
    "\n",
    "        # Retorna True se a conversão foi bem sucedida\n",
    "        return True\n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_columns.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "        return dataframe[keep_columns]\n",
    "    \n",
    "    def calculate_gross_revenue(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a receita bruta de cada cliente com base nas colunas 'Quantity' e 'UnitPrice' e retorna\n",
    "        um DataFrame com as colunas 'CustomerID' e 'gross_revenue'.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases (pd.DataFrame): O DataFrame das compras contendo as colunas 'CustomerID', 'Quantity' e 'UnitPrice'.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante contendo as colunas 'CustomerID' e 'gross_revenue'.\n",
    "        \"\"\"\n",
    "        # Verifica se as colunas necessárias estão presentes no DataFrame de entrada\n",
    "        required_columns = {'CustomerID', 'Quantity', 'UnitPrice'}\n",
    "        missing_columns = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "\n",
    "        # Calcula a receita bruta e agrupa por CustomerID\n",
    "        dataframe_purchases.loc[:, 'gross_revenue'] = dataframe_purchases.loc[:, 'Quantity'] * dataframe_purchases.loc[:, 'UnitPrice']\n",
    "        grouped_df = dataframe_purchases.groupby('CustomerID').agg({'gross_revenue': 'sum'}).reset_index()\n",
    "\n",
    "        return grouped_df\n",
    "\n",
    "    def create_recency(dataframe_purchases: pd.DataFrame, dataframe_filtered: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a recência da última compra para cada cliente.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases (pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "            dataframe_filtered (pd.DataFrame): DataFrame filtrado apenas com as informações dos clientes que desejamos calcular a recência.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'recency_days', indicando a recência em dias da última compra para cada cliente.\n",
    "\n",
    "        \"\"\"\n",
    "        # calcula a data da última compra de cada cliente\n",
    "        df_recency = dataframe_purchases.loc[:, ['CustomerID', 'InvoiceDate']].groupby('CustomerID').max().reset_index()\n",
    "\n",
    "        # calcula a recência em dias da última compra de cada cliente em relação à data mais recente da base de dados filtrada\n",
    "        df_recency.loc[:, 'recency_days'] = (dataframe_filtered['InvoiceDate'].max() - df_recency['InvoiceDate']).dt.days\n",
    "\n",
    "        # retorna o DataFrame apenas com as colunas 'CustomerID' e 'recency_days'\n",
    "        return df_recency[['CustomerID', 'recency_days']]\n",
    "\n",
    "    def create_quantity_purchased(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a quantidade de produtos adquiridos por cada cliente.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases (pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'qty_products', indicando a quantidade de produtos adquiridos por cada cliente.\n",
    "        \"\"\"\n",
    "        # agrupa as informações de compras por CustomerID e conta o número de StockCode para cada grupo\n",
    "        qty_purchased = dataframe_purchases.loc[:, ['CustomerID', 'StockCode']].groupby('CustomerID').count()\n",
    "\n",
    "        # renomeia a coluna StockCode para qty_products e reseta o índice para transformar o CustomerID em uma coluna\n",
    "        qty_purchased = qty_purchased.reset_index().rename(columns={'StockCode': 'qty_products'})\n",
    "\n",
    "        # retorna o DataFrame com as colunas 'CustomerID' e 'qty_products'\n",
    "        return qty_purchased\n",
    "\n",
    "    def create_freq_purchases(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates the purchase frequency of each customer based on the purchase history.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe_purchases : pd.DataFrame\n",
    "            DataFrame with purchase history of each customer, containing columns CustomerID, InvoiceNo, and InvoiceDate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with the purchase frequency of each customer, containing columns CustomerID and frequency.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate time range of purchases for each customer\n",
    "        df_aux = (dataframe_purchases[['CustomerID', 'InvoiceNo', 'InvoiceDate']]\n",
    "                  .drop_duplicates()\n",
    "                  .groupby('CustomerID')\n",
    "                  .agg(max_=('InvoiceDate', 'max'),\n",
    "                       min_=('InvoiceDate', 'min'),\n",
    "                       days_=('InvoiceDate', lambda x: ((x.max() - x.min()).days) + 1),\n",
    "                       buy_=('InvoiceNo', 'count'))\n",
    "                  .reset_index())\n",
    "\n",
    "        # Calculate frequency of purchases for each customer\n",
    "        df_aux['frequency'] = df_aux[['buy_', 'days_']].apply(\n",
    "            lambda x: x['buy_'] / x['days_'] if x['days_'] != 0 else 0, axis=1)\n",
    "\n",
    "        return df_aux\n",
    "\n",
    "    def create_qty_returns(dataframe_returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the total quantity of returned products for each customer.\n",
    "\n",
    "        Args:\n",
    "            dataframe_returns: A pandas DataFrame containing information about returns.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with the total quantity of returned products for each customer.\n",
    "        \"\"\"\n",
    "        # Validate input data\n",
    "        # if dataframe_returns is None:\n",
    "        #     raise ValueError(\"Input DataFrame is empty\")\n",
    "        # if not all(col in dataframe_returns.columns for col in ['CustomerID', 'Quantity']):\n",
    "        #     raise ValueError(\"Input DataFrame must contain 'CustomerID' and 'Quantity' columns\")\n",
    "\n",
    "        # Compute quantity of returns\n",
    "        df_returns = dataframe_returns[['CustomerID', 'Quantity']].groupby('CustomerID').sum().reset_index().rename(columns={'Quantity': 'qty_returns'})\n",
    "        df_returns['qty_returns'] = df_returns['qty_returns']* -1\n",
    "\n",
    "        return df_returns\n",
    "\n",
    "    def run_feature_engineering(dataframe_filtered: pd.DataFrame, dataframe_purchases: pd.DataFrame, dataframe_returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs feature engineering on the input dataframes and returns a new dataframe with the engineered features.\n",
    "\n",
    "        Args:\n",
    "            dataframe_filtered: A pandas DataFrame containing filtered customer order data.\n",
    "            dataframe_purchases: A pandas DataFrame containing customer purchase data.\n",
    "            dataframe_returns: A pandas DataFrame containing customer return data.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with the engineered features for each customer.\n",
    "        \"\"\"\n",
    "        # Check if input dataframes are empty\n",
    "        if dataframe_filtered.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_filtered' is empty\")\n",
    "        if dataframe_purchases.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_purchases' is empty\")\n",
    "        # if dataframe_returns.empty:\n",
    "        #     raise ValueError(\"Input DataFrame 'dataframe_returns' is empty\")\n",
    "\n",
    "        # Check if required columns are present in input dataframes\n",
    "        required_columns = ['CustomerID', 'InvoiceDate', 'StockCode', 'Quantity', 'UnitPrice']\n",
    "        for df, name in zip([dataframe_filtered, dataframe_purchases], ['dataframe_filtered', 'dataframe_purchases']):\n",
    "            missing_columns = set(required_columns) - set(df.columns)\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing columns {missing_columns} in input DataFrame '{name}'\")\n",
    "        if 'CustomerID' not in dataframe_returns.columns:\n",
    "            raise ValueError(\"Column 'CustomerID' not found in input DataFrame 'dataframe_returns'\")\n",
    "        if 'Quantity' not in dataframe_returns.columns:\n",
    "            raise ValueError(\"Column 'Quantity' not found in input DataFrame 'dataframe_returns'\")\n",
    "\n",
    "        # Perform feature engineering\n",
    "        df_fengi = keep_features(dataframe_filtered, ['CustomerID']).drop_duplicates(ignore_index=True)\n",
    "        gross_revenue = calculate_gross_revenue(dataframe_purchases)\n",
    "        df_recency = create_recency(dataframe_purchases, dataframe_filtered)\n",
    "        df_qty_products = create_quantity_purchased(dataframe_purchases)\n",
    "        df_freq = create_freq_purchases(dataframe_purchases)\n",
    "        returns = create_qty_returns(dataframe_returns)\n",
    "\n",
    "        # Merge dataframes\n",
    "        dfs = [df_fengi, gross_revenue, df_recency, df_qty_products, df_freq, returns]\n",
    "        df_fengi = reduce(lambda left,right: pd.merge(left, right, on='CustomerID', how='left'), dfs)\n",
    "\n",
    "        # Fill NaN values\n",
    "        df_fengi['qty_returns'] = df_fengi['qty_returns'].fillna(0)\n",
    "\n",
    "        # Select final features and return dataframe\n",
    "        features = ['CustomerID', 'gross_revenue', 'recency_days', 'qty_products', 'frequency', 'qty_returns']\n",
    "        return keep_features(df_fengi, features).dropna()\n",
    "    \n",
    "    def table_exists(dataset_table_id: str) -> bool:\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        try:\n",
    "            client.get_table(dataset_table_id)  # Make an API request.\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "    \n",
    "    def save_to_bigquery(dataframe: pd.DataFrame, project_name: str, dataset_table_name: str):\n",
    "        client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Load data to BQ\n",
    "        job = client.load_table_from_dataframe(dataframe, dataset_table_name)\n",
    "        job.result()\n",
    "        \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to execute in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query,  or error, if any\n",
    "        \"\"\"\n",
    "        \n",
    "        bq_client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "        bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        # If dry run succeeds without errors, proceed to run query\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        job_id = client_result.job_id\n",
    "\n",
    "        # Wait for query/job to finish running. then get & return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job_id: {job_id}\")\n",
    "        return df\n",
    "\n",
    "    logging.info('Carregando as tabelas da preparacao de dados')\n",
    "    query_filtered = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_FILTERED_TEMP_ID}`\n",
    "                    WHERE InvoiceDate <= CURRENT_TIMESTAMP() \"\"\"\n",
    "    df_filtered = pd.read_gbq(query=query_filtered, \n",
    "                         project_id=PROJECT_NUMBER)\n",
    "    \n",
    "    query_purchases = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_PURCHASES_TEMP_ID}`\n",
    "                    WHERE InvoiceDate <= CURRENT_TIMESTAMP() \"\"\"\n",
    "    df_purchases = pd.read_gbq(query=query_purchases, \n",
    "                         project_id=PROJECT_NUMBER)\n",
    "    \n",
    "    query_returns = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_RETURNS_TEMP_ID}`\"\"\"\n",
    "    df_returns = pd.read_gbq(query=query_returns, \n",
    "                         project_id=PROJECT_NUMBER) \n",
    "    \n",
    "    logging.info('Transformando a coluna InvoiceDate para o tipo DATE')\n",
    "    column_to_date(df_filtered, 'InvoiceDate')\n",
    "    column_to_date(df_purchases, 'InvoiceDate')\n",
    "    \n",
    "    logging.info(f'Iniciando a verificacao de existencia da tabela: {DATASET_ID}.{TABLE_ID}')\n",
    "    # Verifica se a tabela existe\n",
    "    if table_exists(f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"):\n",
    "        logging.info('Tabela existente, inicia insercao de novos dados')\n",
    "        \n",
    "        sql_new_customers = f\"\"\"SELECT\n",
    "                                      DISTINCT CustomerID\n",
    "                                    FROM\n",
    "                                      `{PROJECT_ID}.{DATASET_ID}.{TABLE_RAW_ID}`\n",
    "                                    WHERE\n",
    "                                      InvoiceDate = CURRENT_DATE()\"\"\"\n",
    "        new_customers = pd.read_gbq(sql_new_customers, project_id=PROJECT_NUMBER)['CustomerID'].tolist()\n",
    "       \n",
    "        df_fengi = run_feature_engineering(df_filtered.loc[df_filtered['CustomerID'].isin(new_customers)], \n",
    "                                           df_purchases.loc[df_purchases['CustomerID'].isin(new_customers)], \n",
    "                                           df_returns.loc[df_returns['CustomerID'].isin(new_customers)])\n",
    "        \n",
    "        # Inserir os dados na tabela usando SQL\n",
    "        pandas_gbq.to_gbq(df_fengi, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}', project_id=PROJECT_NUMBER, if_exists='append')\n",
    "        sql_update_new_customer = f\"\"\"\n",
    "                                        UPDATE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "                                        SET values = generate_uuid(),\n",
    "                                        timestamp = current_timestamp()\n",
    "                                        WHERE CustomerID IN {tuple(new_customers)}\"\"\"\n",
    "        logging.info(sql_update_new_customer)\n",
    "        run_bq_query(sql_update_new_customer, project_name=PROJECT_ID)\n",
    "    else:\n",
    "        # Cria a tabela e insere os dados\n",
    "        logging.info('Tabela nao existente, cria a tabela e inicia insercao dos dados')\n",
    "        df_fengi = run_feature_engineering(df_filtered, df_purchases, df_returns)\n",
    "        pandas_gbq.to_gbq(df_fengi, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}', project_id=PROJECT_NUMBER, if_exists='fail')\n",
    "        query = f\"\"\"CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` as (\n",
    "                    SELECT\n",
    "                        *,\n",
    "                        generate_uuid() as values,\n",
    "                        current_timestamp() as timestamp,\n",
    "                    FROM \n",
    "                        `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`);\"\"\"\n",
    "        run_bq_query(query, project_name=PROJECT_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "424e2da0-7543-42ef-b45c-9604dbd846cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. Create feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65749362-7c1e-4fd6-8071-d6b157122144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"pyarrow\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"feature_store.yaml\")\n",
    "def create_feature_store():\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import Feature, Featurestore\n",
    "    #https://medium.com/google-cloud/how-do-you-use-feature-store-in-the-mlops-process-on-vertex-ai-802ddca2cac4\n",
    "    #https://www.youtube.com/watch?v=jXD8Sfx4hvQ\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    \n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_ID = 'dados_engenharia_features'\n",
    "    FEATURESTORE_ID=\"ecommerce_feature_store\"\n",
    "    VALUES_ENTITY_ID = \"values\"\n",
    "    # fazer no python VALUES_BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    FEATURE_TIME = 'timestamp'\n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_number, location= REGION)\n",
    "    \n",
    "    try:\n",
    "        # Checks if there is already a Featurestore\n",
    "        ecommerce_feature_store = aiplatform.Featurestore(f\"{FEATURESTORE_ID}\")\n",
    "        logging.info(f\"\"\"A feature store {FEATURESTORE_ID} ja existe.\"\"\")\n",
    "    except:\n",
    "        # Creates a Featurestore\n",
    "        logging.info(f\"\"\"Criando a feature store: {FEATURESTORE_ID}.\"\"\")\n",
    "        ecommerce_feature_store = aiplatform.Featurestore.create(\n",
    "            featurestore_id=f\"{FEATURESTORE_ID}\",\n",
    "            online_store_fixed_node_count=1,\n",
    "            sync=True,\n",
    "        )\n",
    "        \n",
    "    try:\n",
    "        # get entity type, if it already exists\n",
    "        values_entity_type = ecommerce_feature_store.get_entity_type(entity_type_id=VALUES_ENTITY_ID)\n",
    "    except:\n",
    "        # else, create entity type\n",
    "        values_entity_type = ecommerce_feature_store.create_entity_type(\n",
    "            entity_type_id=VALUES_ENTITY_ID, description=\"Values Entity\", sync=True\n",
    "        )\n",
    "    \n",
    "    values_feature_configs = {\n",
    "                                \"gross_revenue\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Gross Revenue\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"recency_days\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Recency Days\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"qty_products\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Quantity products\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"frequency\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Frequency\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"qty_returns\": {\n",
    "                                    \"value_type\": \"INT64\",\n",
    "                                    \"description\": \"Quantity returns\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                            }}\n",
    "\n",
    "    values_feature_ids = values_entity_type.batch_create_features(\n",
    "        feature_configs=values_feature_configs, sync=True\n",
    "    )\n",
    "    \n",
    "    VALUES_FEATURES_IDS = [feature.name for feature in values_feature_ids.list_features()]\n",
    "    \n",
    "    logging.info(f\"\"\"Ingerindo os dados na feature store: {FEATURESTORE_ID}.\"\"\")\n",
    "    values_entity_type.ingest_from_bq(\n",
    "                                        feature_ids=VALUES_FEATURES_IDS,\n",
    "                                        feature_time=FEATURE_TIME,\n",
    "                                        bq_source_uri=VALUES_BQ_SOURCE_URI,\n",
    "                                        entity_id_field=VALUES_ENTITY_ID,\n",
    "                                        disable_online_serving=True,\n",
    "                                        worker_count=2,\n",
    "                                        sync=True,\n",
    "                                    )\n",
    "    # enable api: https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=343941956592%22\n",
    "    #https://aiinpractice.com/gcp-mlops-vertex-ai-feature-store/\n",
    "    #https://medium.com/hacking-talent/vertexais-feature-store-for-dummies-3d798b45ece4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f061a47-7b8b-4ba8-b167-f18163ed454c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. Batch serving Feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99b11ab-608e-4755-a7a4-4d88420f0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\",\n",
    "                                \"google-cloud-bigquery\", \n",
    "                                \"db-dtypes\",\n",
    "                                \"pandas\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"batch_serve_fs.yaml\")\n",
    "def create_batch_serve_fs():\n",
    "    import os\n",
    "    import logging\n",
    "    from typing import Union\n",
    "    \n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    REGION = \"us-central1\"\n",
    "    TABLE_INSTACES_ID = \"read_instances\"\n",
    "    TABLE_ID = 'dados_engenharia_features'\n",
    "    SERVING_FEATURE_IDS = {\"values\": [\"*\"]}\n",
    "    TABLE_TRAIN_ID = \"dados_treinamento\"\n",
    "    TRAIN_TABLE_URI = f\"bq://{PROJECT_ID}.{DATASET_ID}.{TABLE_TRAIN_ID}\"\n",
    "    FEATURE_STORE_NAME = 'ecommerce_feature_store'\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_number, location = REGION)\n",
    "    \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to execute in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query,  or error, if any\n",
    "        \"\"\"\n",
    "        \n",
    "        bq_client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "        bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        # If dry run succeeds without errors, proceed to run query\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        job_id = client_result.job_id\n",
    "\n",
    "        # Wait for query/job to finish running. then get & return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job_id: {job_id}\")\n",
    "        return df\n",
    "\n",
    "    read_instances_query = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{TABLE_INSTACES_ID}` as (\n",
    "                    SELECT   \n",
    "                        values,\n",
    "                        timestamp,\n",
    "                    FROM \n",
    "                        `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` \n",
    "                );\n",
    "                \"\"\"\n",
    "    \n",
    "    logging.info(\"Criando a tabela de instancia\")\n",
    "    run_bq_query(read_instances_query, project_name=PROJECT_ID)\n",
    "    \n",
    "    logging.info(f\"Iniciando o fornecimento das features da: {FEATURE_STORE_NAME}\")\n",
    "    ecommerce_feature_store = aiplatform.Featurestore(featurestore_name=FEATURE_STORE_NAME)\n",
    "    \n",
    "    logging.info(f\"Executando o comando para o destino: {TRAIN_TABLE_URI} a partir da tabela: {TABLE_INSTACES_ID}\")\n",
    "    ecommerce_feature_store.batch_serve_to_bq(\n",
    "        bq_destination_output_uri=TRAIN_TABLE_URI,\n",
    "        serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "        read_instances_uri=f\"bq://{PROJECT_ID}.{DATASET_ID}.{TABLE_INSTACES_ID}\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dd07a92-8d6b-43fc-b0e9-598a12ab111e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46997d97-fb62-4a37-8929-793f9eb2c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \n",
    "                                \"pandas\", \n",
    "                                \"pyarrow\", \n",
    "                                \"scikit-learn\",\n",
    "                                \"google-cloud-bigquery\", \n",
    "                                \"db-dtypes\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"model_train.yaml\")\n",
    "def model_train(model: Output[Artifact]):\n",
    "    import os\n",
    "    import pickle\n",
    "    import pathlib\n",
    "    import logging\n",
    "    import datetime\n",
    "    from typing import Union\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.cluster import KMeans\n",
    "    from google.cloud import aiplatform\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    logging.info(\"Iniciando o componente\")\n",
    "    \n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    REGION = \"us-central1\"\n",
    "    TABLE_TRAIN_ID = \"dados_treinamento\"\n",
    "    FEATURES = ['qty_products', 'qty_returns', 'frequency','recency_days']\n",
    "    TARGET = 'gross_revenue'\n",
    "    DEPLOY_VERSION = \"sklearn-cpu.1-0\"\n",
    "    FRAMEWORK = \"scikit-learn\"\n",
    "    REGION_SPLITTED = \"us-central1\".split(\"-\")[0]\n",
    "    DEPLOY_IMAGE = f\"{REGION_SPLITTED}-docker.pkg.dev/vertex-ai/prediction/{DEPLOY_VERSION}:latest\"\n",
    "    MODEL_NAME = 'model.pkl'\n",
    "    N_CLUSTERS = 8\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_number, location = REGION)\n",
    "    \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to execute in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query,  or error, if any\n",
    "        \"\"\"\n",
    "        \n",
    "        bq_client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "        bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        # If dry run succeeds without errors, proceed to run query\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        job_id = client_result.job_id\n",
    "\n",
    "        # Wait for query/job to finish running. then get & return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job_id: {job_id}\")\n",
    "        return df\n",
    "    \n",
    "    logging.info(\"Carregando dados para o treinamento\")\n",
    "    df = run_bq_query(f\"select * from `{PROJECT_ID}.{DATASET_ID}.{TABLE_TRAIN_ID}`\", project_name=PROJECT_ID)\n",
    "    \n",
    "    logging.info(\"Iniciando o treinamento\")\n",
    "      \n",
    "    X = df[FEATURES].copy()\n",
    "    y = df[TARGET]\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"pca\", PCA(n_components=2)),\n",
    "            (\"scaler\", scaler),\n",
    "            (\"clustering\", KMeans(n_clusters=N_CLUSTERS, random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model_pipeline.fit(X, y)\n",
    "    \n",
    "    logging.info(\"Criando o modelo output\")\n",
    "    model.metadata[\"framework\"] = FRAMEWORK\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": DEPLOY_IMAGE\n",
    "    }\n",
    "    \n",
    "\n",
    "    file_name = model.path + f\"/{MODEL_NAME}\"\n",
    "    \n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26698921-c5c0-4e69-8c4e-0174b5681f4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Predict batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f5ddc4-2d27-48c7-9325-8ca64d98ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\",\n",
    "                               \"google-cloud-bigquery\", \n",
    "                                \"db-dtypes\", \n",
    "                                \"pandas\", \n",
    "                                \"scikit-learn\",\n",
    "                                \"pandas-gbq\"\n",
    "                               ],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"batch_prediction.yaml\")\n",
    "def batch_prediction(model: Input[Model]):\n",
    "    import os\n",
    "    import pickle\n",
    "    import logging\n",
    "    \n",
    "    import pandas_gbq\n",
    "    import pandas as pd\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    REGION = \"us-central1\"\n",
    "    TABLE_TO_PREDICT_ID = \"dados_treinamento\"\n",
    "    MODEL_NAME = 'model.pkl'\n",
    "    TABLE_SAVE_PREDICTIONS_ID = \"dados_preditos\"\n",
    "    FEATURES = ['qty_products', 'qty_returns', 'frequency','recency_days'] #mesma ordem do treinamento\n",
    "    SQL_TO_PREDICT_DATA = f\"\"\"SELECT * \n",
    "                        FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_TO_PREDICT_ID}`\"\"\"\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    aiplatform.init(project = project_number, location = REGION)\n",
    "\n",
    "    logging.info(\"Iniciando o componente\")\n",
    "    \n",
    "    logging.info(f\"Carregando o modelo: {MODEL_NAME}\")\n",
    "    file_name = model.path + f\"/{MODEL_NAME}\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "    \n",
    "    logging.info(\"Carregando dados a serem preditos\")\n",
    "    predict_data = pd.read_gbq(SQL_TO_PREDICT_DATA, project_id=project_number)\n",
    "    \n",
    "    logging.info(f\"Iniciando a predicao dos dados: {PROJECT_ID}.{DATASET_ID}.{TABLE_TO_PREDICT_ID}\")\n",
    "    labels = model_pipeline.predict(predict_data[FEATURES])\n",
    "    predict_data['Clusters'] = labels\n",
    "    \n",
    "    logging.info(f\"Substituindo os dados preditos: {PROJECT_ID}.{DATASET_ID}.{TABLE_SAVE_PREDICTIONS_ID}\")\n",
    "    pandas_gbq.to_gbq(predict_data, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_SAVE_PREDICTIONS_ID}', project_id=project_number, if_exists='replace')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a1ad2a-1295-481e-99f4-57a5f39b5cdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8. Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2ca7fa-59f9-4c12-939c-cff4680a0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"gs://pipeline-ecommerce-teste/\"\n",
    "\n",
    "@pipeline(pipeline_root=PIPELINE_ROOT,\n",
    "         name=\"ecommerce-pipeline\")\n",
    "def ecommerce_pipeline():\n",
    "    PROJECT_ID = 'gcp-vertex'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    REGION = \"us-central1\"\n",
    "    MODEL_NAME = \"ecommerce-clustering\"\n",
    "    \n",
    "    dataset_op = get_data()\n",
    "    data_prep_op = data_preparation()\n",
    "    data_prep_op.after(dataset_op)\n",
    "    \n",
    "    feature_engineering_op = feature_engineering()    \n",
    "    feature_engineering_op.after(data_prep_op)\n",
    "    \n",
    "    feature_store_op = create_feature_store()\n",
    "    feature_store_op.after(feature_engineering_op)\n",
    "    \n",
    "    batch_serve_fs_op = create_batch_serve_fs()\n",
    "    batch_serve_fs_op.after(feature_store_op)\n",
    "    \n",
    "    model_train_op = model_train()\n",
    "    model_train_op.after(batch_serve_fs_op)\n",
    "    \n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=f\"{MODEL_NAME}\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_train_op)\n",
    "    \n",
    "#     endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "#         project=PROJECT_ID,\n",
    "#         location=REGION,\n",
    "#         display_name=f\"{MODEL_NAME}-endpoint\",\n",
    "#     )\n",
    "    \n",
    "#     model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "#         endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "#         model=model_upload_op.outputs[\"model\"],\n",
    "#         deployed_model_display_name=f\"{MODEL_NAME}\",\n",
    "#         dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "#         dedicated_resources_min_replica_count=1,\n",
    "#         dedicated_resources_max_replica_count=1,\n",
    "#     )\n",
    "    \n",
    "    batch_predict_op = batch_prediction(model=model_train_op.outputs[\"model\"])\n",
    "    # batch_predict_op.after(model_upload_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f866cdd-0acd-4b62-b2fb-3bcc93fa7761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=ecommerce_pipeline,\n",
    "    package_path='ecommerce_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bd6872-e460-4eda-a71e-76823251124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/ecommerce-pipeline-20230612174703?project=343941956592\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/343941956592/locations/us-central1/pipelineJobs/ecommerce-pipeline-20230612174703\n"
     ]
    }
   ],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"ecommerce-pipeline\",\n",
    "    template_path=\"ecommerce_pipeline.json\",\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdaa82b9-fe14-4a0a-9b83-e38632692dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a2152c-8e18-4382-9822-cf753ad44e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Featurestore : projects/343941956592/locations/us-central1/featurestores/ecommerce_feature_store\n",
      "Delete Featurestore  backing LRO: projects/343941956592/locations/us-central1/operations/3207197559055450112\n",
      "Featurestore deleted. . Resource name: projects/343941956592/locations/us-central1/featurestores/ecommerce_feature_store\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud.aiplatform import Feature, Featurestore\n",
    "# fs = Featurestore(\n",
    "#     featurestore_name=\"ecommerce_feature_store\",\n",
    "#     project=\"gcp-vertex\",\n",
    "#     location=\"us-central1\",\n",
    "# )\n",
    "# fs.delete(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8bd89-2bca-44ab-a4d3-13b852fd879e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
